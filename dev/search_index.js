var documenterSearchIndex = {"docs":
[{"location":"noise_types/#Noise-types","page":"Noise types","title":"Noise types","text":"","category":"section"},{"location":"noise_types/#Isotropic-noise","page":"Noise types","title":"Isotropic noise","text":"","category":"section"},{"location":"noise_types/","page":"Noise types","title":"Noise types","text":"By default, Consensus-Based Optimisation uses so-called isotropic noise (option noise = :IsotropicNoise), given by","category":"page"},{"location":"noise_types/","page":"Noise types","title":"Noise types","text":"mathrmdx_t^i =\ncdots\n+ sqrt2sigma^2 \nleft x_t^i - c_alpha(x_t) right mathrmdB_t^i","category":"page"},{"location":"noise_types/","page":"Noise types","title":"Noise types","text":"where B_t^i are independent Brownian motions in D dimensions. The intensity of the noise depends on the distance of each particle to the consensus point, left x_t^i - c_alpha(x_t) right.","category":"page"},{"location":"noise_types/","page":"Noise types","title":"Noise types","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nconfig = (; D = 2, noise = :IsotropicNoise)\nminimise(f, config)","category":"page"},{"location":"noise_types/#Anisotropic-noise","page":"Noise types","title":"Anisotropic noise","text":"","category":"section"},{"location":"noise_types/","page":"Noise types","title":"Noise types","text":"ConsensusBasedX.jl also offers anisotropic noise, given by ","category":"page"},{"location":"noise_types/","page":"Noise types","title":"Noise types","text":"mathrmdx_t^i =\ncdots\n+ sqrt2sigma^2 \noperatorname*diag left( x_t^i - c_alpha(x_t) right) mathrmdB_t^i","category":"page"},{"location":"noise_types/","page":"Noise types","title":"Noise types","text":"The intensity of the noise now varies along each dimension. This can be selected with the option noise = :AnisotropicNoise.","category":"page"},{"location":"noise_types/","page":"Noise types","title":"Noise types","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nconfig = (; D = 2, noise = :AnisotropicNoise)\nminimise(f, config)","category":"page"},{"location":"stopping_criteria/#Stopping-criteria","page":"Stopping criteria","title":"Stopping criteria","text":"","category":"section"},{"location":"stopping_criteria/","page":"Stopping criteria","title":"Stopping criteria","text":"You can apply any of these criteria by passing them as keywords to the minimise routine.","category":"page"},{"location":"stopping_criteria/#Energy-threshold","page":"Stopping criteria","title":"Energy threshold","text":"","category":"section"},{"location":"stopping_criteria/","page":"Stopping criteria","title":"Stopping criteria","text":"energy_threshold::Real = -Inf sets a stopping threshold for the value of f(v), where v is the current consensus point. For each ensemble, if f(v) < energy_threshold, the minimisation stops.","category":"page"},{"location":"stopping_criteria/","page":"Stopping criteria","title":"Stopping criteria","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nconfig = (; D = 2, energy_threshold = 1e-2)\nminimise(f, config) # should be close to [1, 1]","category":"page"},{"location":"stopping_criteria/#Energy-tolerance","page":"Stopping criteria","title":"Energy tolerance","text":"","category":"section"},{"location":"stopping_criteria/","page":"Stopping criteria","title":"Stopping criteria","text":"energy_tolerance::Real = 1e-8 dictates a tolerance for the change in f(v), where v is the current consensus point. For each ensemble, if abs(f(v) - f(v_prev)) < energy_tolerance, where v_prev is the previous consensus point, the minimisation stops.","category":"page"},{"location":"stopping_criteria/","page":"Stopping criteria","title":"Stopping criteria","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nconfig = (; D = 2, energy_tolerance = 1e-4)\nminimise(f, config) # should be close to [1, 1]","category":"page"},{"location":"stopping_criteria/#Max-evaluations","page":"Stopping criteria","title":"Max evaluations","text":"","category":"section"},{"location":"stopping_criteria/","page":"Stopping criteria","title":"Stopping criteria","text":"max_evaluations::Real = Inf determines the maximum number of times f may be evaluated by the minimisation. If the value is exceeded, the minimisation stops.","category":"page"},{"location":"stopping_criteria/","page":"Stopping criteria","title":"Stopping criteria","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nconfig = (; D = 2, max_evaluations = 1000)\nminimise(f, config) # should be close to [1, 1]","category":"page"},{"location":"stopping_criteria/#Max-iterations","page":"Stopping criteria","title":"Max iterations","text":"","category":"section"},{"location":"stopping_criteria/","page":"Stopping criteria","title":"Stopping criteria","text":"max_iterations::Real = 1000 specifies the maximal number of iterations that the time integrator can perform. If the number is reached, the minimisation stops.","category":"page"},{"location":"stopping_criteria/","page":"Stopping criteria","title":"Stopping criteria","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nconfig = (; D = 2, max_iterations = 80)\nminimise(f, config) # should be close to [1, 1]","category":"page"},{"location":"stopping_criteria/#Max-time","page":"Stopping criteria","title":"Max time","text":"","category":"section"},{"location":"stopping_criteria/","page":"Stopping criteria","title":"Stopping criteria","text":"max_time::Real = Inf determines the maximal solution time (the corresponding SDE is solved from time 0 until time max_time). If the number of iterations times Δt surpasses this value, the minimisation stops. ","category":"page"},{"location":"stopping_criteria/","page":"Stopping criteria","title":"Stopping criteria","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nconfig = (; D = 2, max_time = 5)\nminimise(f, config) # should be close to [1, 1]","category":"page"},{"location":"consensus-based_sampling/#Consensus-Based-Sampling","page":"Consensus-based sampling","title":"Consensus-Based Sampling","text":"","category":"section"},{"location":"consensus-based_sampling/","page":"Consensus-based sampling","title":"Consensus-based sampling","text":"Consensus-Based Optimisation (CBO) solves the problem of minimising a function f by computing weighted means of the associated \"Gibbs-like\" distribution exp(-alpha f(x)). A related problem is the sampling problem:","category":"page"},{"location":"consensus-based_sampling/","page":"Consensus-based sampling","title":"Consensus-based sampling","text":"note: Sampling problem\nGiven a target distribution g(x)=mathbbR^D rightarrow mathbbR^+, find N independent samples X^i in mathbbR^D from the random variable X with operatorname*Law(X) = g. Informally, this means that a normalised histogram of X^i will resemble g(x) when N is large.","category":"page"},{"location":"consensus-based_sampling/","page":"Consensus-based sampling","title":"Consensus-based sampling","text":"Consensus-based sampling (CBS) is an approach to the sampling problem in the case g(x) propto exp(-alpha f(x)). This is a common scenario that arises, for instance, in Bayesian inference. It uses the same consensus point as CBO, ","category":"page"},{"location":"consensus-based_sampling/","page":"Consensus-based sampling","title":"Consensus-based sampling","text":"c_alpha(x_t) =\nfrac1 sum_i=1^N omega_alpha(x_t^i) \nsum_i=1^N x_t^i  omega_alpha(x_t^i)\nquadtextwherequad\nomega_alpha(cdot) = mathrmexp(-alpha f(cdot))","category":"page"},{"location":"consensus-based_sampling/","page":"Consensus-based sampling","title":"Consensus-based sampling","text":"for some alpha0. It additionally defines a weighted covariance matrix,","category":"page"},{"location":"consensus-based_sampling/","page":"Consensus-based sampling","title":"Consensus-based sampling","text":"V_alpha(x_t) =\nfrac1 sum_i=1^N omega_alpha(x_t^i) \nsum_i=1^N\nleft( x_t^i - c_alpha(x_t) right) otimes left( x_t^i - c_alpha(x_t) right)\n omega_alpha(x_t^i)","category":"page"},{"location":"consensus-based_sampling/","page":"Consensus-based sampling","title":"Consensus-based sampling","text":"On each method iteration, the particles evolve in time following the rule","category":"page"},{"location":"consensus-based_sampling/","page":"Consensus-based sampling","title":"Consensus-based sampling","text":"x_n+1^i =\nunderbrace\nbeta x_n^i + (1-beta) c_alpha(x_n)\n_\ntextconsensus drift\n\n+ sqrtfrac 1 - beta^2  lambda  underbrace\nV_alpha^frac12(x_n) xi^i\n_\ntextscaled diffusion\n","category":"page"},{"location":"consensus-based_sampling/","page":"Consensus-based sampling","title":"Consensus-based sampling","text":"where betain(01) is an interpolation parameter, and where xi^i are independent standard normal random variables in D dimensions. The consensus drift is a deterministic interpolation towards the consensus point; meanwhile, the scaled diffusion is a stochastic term that controls the variance of x_n^i. This update rule can be understood as a numerical scheme for an SDE with a time step Delta t, where beta = exp(-Delta t). The corresponding SDE would be ","category":"page"},{"location":"consensus-based_sampling/","page":"Consensus-based sampling","title":"Consensus-based sampling","text":"mathrmdx_t^i =\n- underbrace\nleft( x_t^i - c_alpha(x_t) right) mathrmdt\n_\ntextconsensus drift\n\n+ sqrt2lambda^-1 underbrace\nV_alpha^frac12(x_t) mathrmdB_t^i\n_\ntextscaled diffusion\n","category":"page"},{"location":"consensus-based_sampling/","page":"Consensus-based sampling","title":"Consensus-based sampling","text":"The matrix V_alpha^frac12(x_t) is the matrix square-root of the covariance V_alpha(x_t), in the sense that V_alpha = V_alpha^frac12 left( V_alpha^frac12 right) ^T. An alternative (distributionally equivalent) covariance square-root is given in A. Garbuno-Inigo, N. Nüsken, and S. Reich (2020), and also implemented in ConsensusBasedX.jl (see Root-covariance types). By default, the version with the best performance is selected, as a function of D and N.","category":"page"},{"location":"consensus-based_sampling/","page":"Consensus-based sampling","title":"Consensus-based sampling","text":"The parameter lambda controls the overall behaviour of the algorithm. If lambda = (1+alpha)^-1, the particles will converge in time towards the distribution exp(-alpha f(x)); therefore, their final positions are approximately samples of the target distribution. If lambda = 1, the particles will converge towards the consensus point, which will be an approximation of the global minimiser of f, just as in CBO.","category":"page"},{"location":"consensus-based_sampling/","page":"Consensus-based sampling","title":"Consensus-based sampling","text":"For additional details, see J. A. Carrillo, F. Hoffmann, A. M. Stuart, and U. Vaes (2022). Note that, in their notation, the roles of alpha and beta are switched.","category":"page"},{"location":"consensus-based_optimisation/#Consensus-Based-Optimisation","page":"Consensus-based optimisation","title":"Consensus-Based Optimisation","text":"","category":"section"},{"location":"consensus-based_optimisation/","page":"Consensus-based optimisation","title":"Consensus-based optimisation","text":"Consensus-based optimisation (CBO) is an approach to solve the global minimisation problem:","category":"page"},{"location":"consensus-based_optimisation/","page":"Consensus-based optimisation","title":"Consensus-based optimisation","text":"note: Global minimisation problem\nGiven a (continuous) objective function f(x)mathbbR^D rightarrow mathbbR, findx^* = operatorname*argmin_xinmathbbR^D f(x)i.e., find the point where f takes its lowest value.","category":"page"},{"location":"consensus-based_optimisation/","page":"Consensus-based optimisation","title":"Consensus-based optimisation","text":"CBO uses a finite number N of agents (driven particles), x_t=(x_t^1dotsx_t^N), to explore the landscape of f without evaluating any of its derivatives. At each time t, the agents evaluate the objective function at their position, f(x_t^i), and define a  consensus point c_alpha. This point is an approximation of the global minimiser x^*, and is constructed by weighing each agent's position against a \"Gibbs-like\" distribution, exp(-alpha f(x)):","category":"page"},{"location":"consensus-based_optimisation/","page":"Consensus-based optimisation","title":"Consensus-based optimisation","text":"c_alpha(x_t) =\nfrac1 sum_i=1^N omega_alpha(x_t^i) \nsum_i=1^N x_t^i  omega_alpha(x_t^i)\nquadtextwherequad\nomega_alpha(cdot) = mathrmexp(-alpha f(cdot))","category":"page"},{"location":"consensus-based_optimisation/","page":"Consensus-based optimisation","title":"Consensus-based optimisation","text":"for some alpha0. The exponential weights in the definition favour those points x_t^i where f(x_t^i) is lowest, and comparatively ignore the rest. If all the found values of the objective function are approximately the same, c_alpha(x_t) is roughly an arithmetic mean; if, instead, one particle is much better than the rest, c_alpha(x_t) will be very close to its position.","category":"page"},{"location":"consensus-based_optimisation/","page":"Consensus-based optimisation","title":"Consensus-based optimisation","text":"Once the consensus point is defined, the particles evolve in time following the stochastic differential equation (SDE)","category":"page"},{"location":"consensus-based_optimisation/","page":"Consensus-based optimisation","title":"Consensus-based optimisation","text":"mathrmdx_t^i =\n-lambda underbrace\nleft( x_t^i - c_alpha(x_t) right) mathrmdt\n_\ntextconsensus drift\n\n+ sqrt2sigma^2 underbrace\nleft x_t^i - c_alpha(x_t) right mathrmdB_t^i\n_\ntextscaled diffusion\n","category":"page"},{"location":"consensus-based_optimisation/","page":"Consensus-based optimisation","title":"Consensus-based optimisation","text":"where lambda and sigma are positive parameters, and where B_t^i are independent Brownian motions in D dimensions. The consensus drift is a deterministic term which drives each agent towards the consensus point, at rate lambda; meanwhile, the scaled diffusion is a stochastic term that encourages exploration.","category":"page"},{"location":"consensus-based_optimisation/","page":"Consensus-based optimisation","title":"Consensus-based optimisation","text":"For additional details, see R. Pinnau, C. Totzeck, O. Tse, and S. Martin (2017).","category":"page"},{"location":"performance_benchmarking/#Performance-and-benchmarking","page":"Performance and benchmarking","title":"Performance and benchmarking","text":"","category":"section"},{"location":"performance_benchmarking/","page":"Performance and benchmarking","title":"Performance and benchmarking","text":"ConsensusBasedX.jl has been developed with performance in mind. As such, it follows the recommended code patterns, such as avoiding global variables, or keeping memory allocation outside of performance-critical functions.","category":"page"},{"location":"performance_benchmarking/","page":"Performance and benchmarking","title":"Performance and benchmarking","text":"In order to maintain good performance, it's important that your function f also follows these patterns. We recommend the performance tips section of the Julia documentation. You should benchmark f, paying attention to memory allocations.","category":"page"},{"location":"performance_benchmarking/","page":"Performance and benchmarking","title":"Performance and benchmarking","text":"tip: Tip\nYou should use the BenchmarkTools.jl package to benchmark f.","category":"page"},{"location":"performance_benchmarking/","page":"Performance and benchmarking","title":"Performance and benchmarking","text":"Once you have benchmarked f, you might want to test its performance within ConsensusBasedX.jl. You could run","category":"page"},{"location":"performance_benchmarking/","page":"Performance and benchmarking","title":"Performance and benchmarking","text":"config = (; D = 2)\n@time minimise(f, config)","category":"page"},{"location":"performance_benchmarking/","page":"Performance and benchmarking","title":"Performance and benchmarking","text":"but the output of @time will be noisy, as it will include the time and memory allocations due to the caches created by ConsensusBasedX.jl. ","category":"page"},{"location":"performance_benchmarking/","page":"Performance and benchmarking","title":"Performance and benchmarking","text":"If, instead, you want to test only the performance-critical parts of the code, you can run the routine in benchmark mode:","category":"page"},{"location":"performance_benchmarking/","page":"Performance and benchmarking","title":"Performance and benchmarking","text":"config = (; D = 2, benchmark = true)\nminimise(f, config)","category":"page"},{"location":"performance_benchmarking/","page":"Performance and benchmarking","title":"Performance and benchmarking","text":"This will run the beginning of the minimise routine as normal, creating the required caches. However, instead of computing the full particle evolution, it will only calculate a few steps, printing the output of @time to console, and returning the output of @timed.","category":"page"},{"location":"performance_benchmarking/","page":"Performance and benchmarking","title":"Performance and benchmarking","text":"tip: Tip\nThe benchmark mode reports zero allocations with all the Example objectives provided by ConsensusBasedX.jl. Wherever possible, your function should also lead to zero allocations.","category":"page"},{"location":"performance_benchmarking/","page":"Performance and benchmarking","title":"Performance and benchmarking","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nconfig = (; D = 2, benchmark = true)\nminimise(f, config)","category":"page"},{"location":"performance_benchmarking/","page":"Performance and benchmarking","title":"Performance and benchmarking","text":"warning: Warning\nIf you are running Consensus-Based Sampling by calling sample, allocations might occur whenever the root = :SymmetricRoot is automatically selected (see Root-covariance types). To have zero allocations, you must run with the option root = :AsymmetricRoot. Nevertheless, despite the allocations, the root = :SymmetricRoot option offers better performance when N is large (roughly if N > 10 * D).","category":"page"},{"location":"function_minimisation/#Function-minimisation","page":"Function minimisation","title":"Function minimisation","text":"","category":"section"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"The main functionality of ConsensusBasedX.jl is function minimisation via Consensus-Based Optimisation. It assumes you have defined a function f(x::AbstractVector) that takes a single vector argument x of length D = length(x).","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"For instance, if D = 2, you can minimise f by running:","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"minimise(f, D = 2)","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"By default, minimise returns a Vector{Float64} of length D which contains the candidate to the global minimiser of f.","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"note: Note\nYou must always provide D.","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nminimise(f, D = 2) # should be close to [1, 1]","category":"page"},{"location":"function_minimisation/#Using-a-config-object","page":"Function minimisation","title":"Using a config object","text":"","category":"section"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"For more advanced usage, you will select several options. You can pass these as extra keyword arguments to minimise, or you can create a NamedTuple called config and pass that:","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"config = (; D = 2)\nminimise(f, config)","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"note: Note\nIf you pass a Dict instead, it will be converted to a NamedTuple automatically.","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"This is a version of the full-code example above, using config instead:","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nconfig = (; D = 2, N = 20)\nminimise(f, config) # should be close to [1, 1]","category":"page"},{"location":"function_minimisation/#Receiving-extended-output","page":"Function minimisation","title":"Receiving extended output","text":"","category":"section"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"It is possible to receive extended output from minimise by passing the option extended_output = true:","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"config = (; D = 2, extended_output = true)\nminimise(f, config)","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"For more details, see Extended output.","category":"page"},{"location":"function_minimisation/#Maximisation","page":"Function minimisation","title":"Maximisation","text":"","category":"section"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"ConsensusBasedX.jl also defines maximise for convenience. If you call","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"maximise(f, D = 2)","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"or ","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"config = (; D = 2)\nmaximise(f, config)","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"maximise will attempt to define g(x) = -f(x) and call minimise(g, config).","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"These are full-code examples using keywords","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"details: Full example\nusing ConsensusBasedX\nf(x) = -ConsensusBasedX.Ackley(x, shift = 1)\nmaximise(f, D = 2) # should be close to [1, 1]","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"or using config","category":"page"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"details: Full example\nusing ConsensusBasedX\nf(x) = -ConsensusBasedX.Ackley(x, shift = 1)\nconfig = (; D = 2, N = 20)\nmaximise(f, config) # should be close to [1, 1]","category":"page"},{"location":"function_minimisation/#Method-reference","page":"Function minimisation","title":"Method reference","text":"","category":"section"},{"location":"function_minimisation/","page":"Function minimisation","title":"Function minimisation","text":"Pages = [\"function_minimisation.md\"]","category":"page"},{"location":"function_minimisation/#ConsensusBasedX.maximise","page":"Function minimisation","title":"ConsensusBasedX.maximise","text":"maximise(f; keywords...)\n\nmaximise(f, config::NamedTuple)\n\nMaximise the function f using Consensus-Based Optimisation.\n\nAttempts to define x -> -f(x) and calls the minimise routine. This might be better handled directly by the user (see Maximisation).\n\nSee also minimise.\n\n\n\n\n\n","category":"function"},{"location":"function_minimisation/#ConsensusBasedX.minimise","page":"Function minimisation","title":"ConsensusBasedX.minimise","text":"minimise(f; keywords...)\n\nminimise(f, config::NamedTuple)\n\nMinimise the function f using Consensus-Based Optimisation (see Function minimisation).\n\nYou must specify the dimension D of the problem. Other parameters (e.g. the number of particles N or the number of ensembles M) can also be specified; see Summary of options.\n\nExamples\n\nminimise(f, D = 2)\n\nconfig = (; D = 2);\nminimise(f, config)\n\nminimise(f, D = 2, N = 20)\n\nconfig = (; D = 2, N = 20);\nminimise(f, config)\n\n\n\n\n\n","category":"function"},{"location":"summary_options/#Summary-of-options","page":"Summary of options","title":"Summary of options","text":"","category":"section"},{"location":"summary_options/#Basic-options","page":"Summary of options","title":"Basic options","text":"","category":"section"},{"location":"summary_options/","page":"Summary of options","title":"Summary of options","text":"D::Int is the dimension of the problem. This option must always be provided by the user.\nN::Int = 20 is the number of particles.\nM::Int = 1 is the number of ensembles.\nΔt::Real = 0.1 is the time step.\nσ::Real = 1 is the noise strengh.\nλ::Real = 1 is the drift strengh.\nα::Real = 10 is the exponential weight parameter.","category":"page"},{"location":"summary_options/#Initialisation-options","page":"Summary of options","title":"Initialisation options","text":"","category":"section"},{"location":"summary_options/","page":"Summary of options","title":"Summary of options","text":"See Particle initialisation.","category":"page"},{"location":"summary_options/","page":"Summary of options","title":"Summary of options","text":"initialisation = :normal is the default sampling method. You can change it to initialisation = :uniform in order to sample uniformly from a hyperbox around initial_guess.\ninitial_guess::Union{Real,AbstractVector} provides an initial guess for the global minimiser.\ninitial_mean::Union{Real,AbstractVector} customises the mean of the initial distribution of particles. This is an alias for initial_guess.\ninitial_variance::Union{Real,AbstractVector,AbstractMatrix} customises the mean of the initial distribution of particles (if the sampling is normal). It has the alias initial_covariance.\ninitial_radius::Union{Real,AbstractVector} specifies the size of the hyperbox (if the sampling is uniform). You specify initial_diameter instead.\ninitial_particles::AbstractArray{<:Real,3} specifies the initial position of the particles.","category":"page"},{"location":"summary_options/#Stopping-options","page":"Summary of options","title":"Stopping options","text":"","category":"section"},{"location":"summary_options/","page":"Summary of options","title":"Summary of options","text":"See Stopping criteria.","category":"page"},{"location":"summary_options/","page":"Summary of options","title":"Summary of options","text":"energy_threshold::Real = -Inf is the stopping threshold for the value of f.\nenergy_tolerance::Real = 1e-8 is the stopping tolerance for the value of f.\nmax_evaluations::Real = Inf is the maximum number of evaluations of f.\nmax_iterations::Real = 1000 is the maximum number of iterations.\nmax_time::Real = Inf is the maximal  solution time (the corresponding SDE is solved from time 0 until time max_time).","category":"page"},{"location":"summary_options/#Advanced-options","page":"Summary of options","title":"Advanced options","text":"","category":"section"},{"location":"summary_options/","page":"Summary of options","title":"Summary of options","text":"noise = :IsotropicNoise controls the type of noise, see Noise types.\nbenchmark::Bool = false controls the benchmark behaviour. benchmark = true runs the ParticleDynamic on benchmark mode, see Performance and benchmarking.\nextended_output::Bool = false controls the output, and by default returns only the computed minimiser. extended_output = true returns additional information, see Extended output.\nparallelisation = :NoParallelisation controls the parallelisation of the minimise routine, switched off by default. parallelisation=:EnsembleParallelisation enables parallelisation, see  Parallelisation.\nverbosity::Int = 0 is the verbosity level. verbosity = 0 produces no output to console. verbosity = 1 produces some output. ","category":"page"},{"location":"summary_options/#Consensus-based-sampling-options","page":"Summary of options","title":"Consensus-based sampling options","text":"","category":"section"},{"location":"summary_options/","page":"Summary of options","title":"Summary of options","text":"CBS_mode = :sampling controls the mode of consensus-based sampling. If you want to perform a minimisation, pass CBS_mode = :minimise instead.\nroot = :SymmetricRoot controls the type of root-covariance matrix, see Root-covariance types.","category":"page"},{"location":"extended_output/#Extended-output","page":"Extended output","title":"Extended output","text":"","category":"section"},{"location":"extended_output/","page":"Extended output","title":"Extended output","text":"By default, the minimise and sample routines return their best guess for the global minimiser of the function f. However, it is possible to access the extended output by passing the extended_output = true option.","category":"page"},{"location":"extended_output/","page":"Extended output","title":"Extended output","text":"When calling minimise, the extended output is a NamedTuple which contains:","category":"page"},{"location":"extended_output/","page":"Extended output","title":"Extended output","text":"minimiser, a Vector{Float64}, the candidate to the global minimiser of f;\nensemble_minimiser, a Vector of the M minimisers computed by each ensemble. minimiser is equal to their mean;\ninitial_particles, the initial position of the particles, see Particle initialisation;\nfinal_particles, the final position of the particles.","category":"page"},{"location":"extended_output/","page":"Extended output","title":"Extended output","text":"When calling sample, the extended output also includes sample, which is an alias for final_particles. The final position of the particles is the distribution sample when running with CBS_mode = :sampling.","category":"page"},{"location":"extended_output/","page":"Extended output","title":"Extended output","text":"In both cases, certain low-level caches are included as well:","category":"page"},{"location":"extended_output/","page":"Extended output","title":"Extended output","text":"method, by default a ConsensusBasedOptimisation object;\nmethod_cache, by default a ConsensusBasedOptimisationCache object;\nparticle_dynamic, by default a ParticleDynamic object;\nparticle_dynamic_cache, by default a ParticleDynamicCache object. ","category":"page"},{"location":"extended_output/","page":"Extended output","title":"Extended output","text":"These objects are part of the Low-level interface.","category":"page"},{"location":"root-covariance_types/#Root-covariance-types","page":"Root-covariance types","title":"Root-covariance types","text":"","category":"section"},{"location":"root-covariance_types/","page":"Root-covariance types","title":"Root-covariance types","text":"Consensus-Based Sampling uses the usual matrix square root to compute the root-covariance matrix V_alpha^frac12(x_t) (option root = :SymmetricRoot). However, an alternative, non-symmetric, distributionally equivalent approach is given in A. Garbuno-Inigo, N. Nüsken, and S. Reich (2020) (option root = :AsymmetricRoot).","category":"page"},{"location":"root-covariance_types/","page":"Root-covariance types","title":"Root-covariance types","text":"By default, CBS uses root = :AsymmetricRoot if N <= 10 * D, and root = :SymmetricRoot otherwise, as this approach gives (roughly) the best performance.","category":"page"},{"location":"particle_initialisation/#Particle-initialisation","page":"Particle initialisation","title":"Particle initialisation","text":"","category":"section"},{"location":"particle_initialisation/","page":"Particle initialisation","title":"Particle initialisation","text":"ConsensusBasedX.jl needs to initialise particles in order to perform function minimisation.","category":"page"},{"location":"particle_initialisation/#Default-initialisation","page":"Particle initialisation","title":"Default initialisation","text":"","category":"section"},{"location":"particle_initialisation/","page":"Particle initialisation","title":"Particle initialisation","text":"If no options are provided, ConsensusBasedX.jl initialises its particles by sampling a standard normal distribution (a normal distribution with zero mean and unit variance).","category":"page"},{"location":"particle_initialisation/#Initial-guess","page":"Particle initialisation","title":"Initial guess","text":"","category":"section"},{"location":"particle_initialisation/","page":"Particle initialisation","title":"Particle initialisation","text":"If you have an initial guess for the global minimiser of the function f, you can pass the option initial_guess (or initial_mean). This can be a Real, if you want to use the same value for each coordinate of the initial guess, or an AbstractVector of size size(initial_guess) = (D,). The particles will be initialised by sampling a normal distribution with mean initial_guess/initial_mean and unit variance. ","category":"page"},{"location":"particle_initialisation/","page":"Particle initialisation","title":"Particle initialisation","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nconfig = (; D = 2, N = 20, M = 1, initial_guess = [1.1, 0.9])\nminimise(f, config) # should be close to [1, 1]","category":"page"},{"location":"particle_initialisation/#Specify-a-normal-distribution","page":"Particle initialisation","title":"Specify a normal distribution","text":"","category":"section"},{"location":"particle_initialisation/","page":"Particle initialisation","title":"Particle initialisation","text":"If you want to specify the variance of the normal distribution sampled around initial_guess/initial_mean, you can pass the option initial_variance (or initial_covariance). This can be a Real, if you want an isotropic distribution, an AbstractVector of size size(initial_variance) = (D,), if you want to specify the variance along each axis, or an AbstractMatrix of size size(initial_variance) = (D, D), if you want a general multivariate normal distribution.","category":"page"},{"location":"particle_initialisation/","page":"Particle initialisation","title":"Particle initialisation","text":"details: Full example\nusing ConsensusBasedX\n\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\n\nconfig = (; D = 2, N = 20, initial_variance = 5)\nminimise(f, config) # should be close to [1, 1]","category":"page"},{"location":"particle_initialisation/#Specify-a-uniform-distribution","page":"Particle initialisation","title":"Specify a uniform distribution","text":"","category":"section"},{"location":"particle_initialisation/","page":"Particle initialisation","title":"Particle initialisation","text":"You can instead initialise the particles by sampling uniformly from a box around initial_guess/initial_mean. To do so, pass the option initialisation = :uniform.","category":"page"},{"location":"particle_initialisation/","page":"Particle initialisation","title":"Particle initialisation","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nconfig = (; D = 2, N = 20, initialisation = :uniform)\nminimise(f, config) # should be close to [1, 1]","category":"page"},{"location":"particle_initialisation/","page":"Particle initialisation","title":"Particle initialisation","text":"You can specify the radius of the box with the option initial_radius, or the diameter with initial_diameter. This can be a Real, if you want a hypercube, or an AbstractVector of size size(initial_guess) = (D,), if you want a hyperbox with different dimensions along each axis.","category":"page"},{"location":"particle_initialisation/","page":"Particle initialisation","title":"Particle initialisation","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nconfig = (; D = 2, N = 20, initial_radius = 3)\nminimise(f, config) # should be close to [1, 1]","category":"page"},{"location":"particle_initialisation/#Custom-initialisation","page":"Particle initialisation","title":"Custom initialisation","text":"","category":"section"},{"location":"particle_initialisation/","page":"Particle initialisation","title":"Particle initialisation","text":"You can provide the initial position of the particles direcly by passing the option initial_particles. This must be an AbstractArray{<:Real,3} of size (D, N, M).","category":"page"},{"location":"particle_initialisation/","page":"Particle initialisation","title":"Particle initialisation","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nconfig = (; D = 2, N = 20, M = 1, initial_particles = rand(2, 20, 1))\nminimise(f, config) # should be close to [1, 1]","category":"page"},{"location":"particle_initialisation/","page":"Particle initialisation","title":"Particle initialisation","text":"tip: Tip\nIf you are initialising the particles yourself, you might find the Distributions.jl package useful.","category":"page"},{"location":"output_visualisation/#Output-visualisation","page":"Output visualisation","title":"Output visualisation","text":"","category":"section"},{"location":"output_visualisation/","page":"Output visualisation","title":"Output visualisation","text":"The auxiliary package ConsensusBasedXPlots.jl provides routines to visualise the output of minimise for problems in one or two dimensions. These routines are kept as a separate package in order to minimise the dependencies of ConsensusBasedX.jl, since not every user will require visualisation tools.","category":"page"},{"location":"output_visualisation/","page":"Output visualisation","title":"Output visualisation","text":"To plot the output of CBO, simply run minimise with extended_output = true (see Extended output), and then call the plot_CBO method:","category":"page"},{"location":"output_visualisation/","page":"Output visualisation","title":"Output visualisation","text":"out = minimise(f, D = 2, extended_output = true)\nusing ConsensusBasedXPlots\nplot_CBO(out)","category":"page"},{"location":"output_visualisation/","page":"Output visualisation","title":"Output visualisation","text":"Examples in one dimension","category":"page"},{"location":"output_visualisation/","page":"Output visualisation","title":"Output visualisation","text":"details: Full example\nusing ConsensusBasedX, ConsensusBasedXPlots\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nout = minimise(f, D = 1, extended_output = true)\nplot_CBO(out)\nsavefig(\"CBO_1D\")","category":"page"},{"location":"output_visualisation/","page":"Output visualisation","title":"Output visualisation","text":"and in two dimensions","category":"page"},{"location":"output_visualisation/","page":"Output visualisation","title":"Output visualisation","text":"details: Full example\nusing ConsensusBasedX, ConsensusBasedXPlots\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nout = minimise(f, D = 2, extended_output = true)\nplot_CBO(out)\nsavefig(\"CBO_2D\")","category":"page"},{"location":"parallelisation/#Parallelisation","page":"Parallelisation","title":"Parallelisation","text":"","category":"section"},{"location":"parallelisation/","page":"Parallelisation","title":"Parallelisation","text":"Consensus-based optimisation is often used to tackle minimisation problems where f(x) is expensive to evaluate (for instance, parameter estimation in a partial differential equation model). Therefore, ConsensusBasedX.jl does not use parallelisation by default, as it assumes the implementation of f will be parallelised if possible.","category":"page"},{"location":"parallelisation/","page":"Parallelisation","title":"Parallelisation","text":"However, you can enable parallelisation by passing the option parallelisation=:EnsembleParallelisation. With this option, ConsensusBasedX.jl will run each of the M particle ensembles in parallel, using multithreading.","category":"page"},{"location":"parallelisation/","page":"Parallelisation","title":"Parallelisation","text":"warning: Warning\nParallelisation leads to memory allocations which cannot be avoided, as there is overhead associated with distributing the tasks. If you activate parallelisation, and then run minimise in benchmark mode (see Performance and benchmarking), you will detect some allocations, and this is expected.","category":"page"},{"location":"parallelisation/","page":"Parallelisation","title":"Parallelisation","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nconfig = (; D = 2, M = 5, parallelisation = :EnsembleParallelisation)\nminimise(f, config)","category":"page"},{"location":"low_level_examples/#Low-level-interface-examples","page":"Low-level interface examples","title":"Low-level interface examples","text":"","category":"section"},{"location":"low_level_examples/","page":"Low-level interface examples","title":"Low-level interface examples","text":"We provide two low-level interface examples for the convenience of advanced users.","category":"page"},{"location":"low_level_examples/#Manual-method-definition","page":"Low-level interface examples","title":"Manual method definition","text":"","category":"section"},{"location":"low_level_examples/","page":"Low-level interface examples","title":"Low-level interface examples","text":"This example bypasses the minimise interface, and defines the ParticleDynamic and ConsensusBasedOptimisation structs directly. However, ConsensusBasedX.construct_particle_dynamic_cache is used to construct the caches:","category":"page"},{"location":"low_level_examples/","page":"Low-level interface examples","title":"Low-level interface examples","text":"details: Full example\nusing ConsensusBasedX, ConsensusBasedX.ConsensusBasedXLowLevel\n\nconfig =\n  (; D = 2, N = 20, M = 1, α = 10.0, λ = 1.0, σ = 1.0, Δt = 0.1, verbosity = 0)\n\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\n\nX₀ = [[rand(config.D) for n ∈ 1:(config.N)] for m ∈ 1:(config.M)]\n\ncorrection = HeavisideCorrection()\nnoise = IsotropicNoise\nmethod =\n  ConsensusBasedOptimisation(f, correction, noise, config.α, config.λ, config.σ)\n\nΔt = 0.1\nparticle_dynamic = ParticleDynamic(method, Δt)\n\nparticle_dynamic_cache =\n  construct_particle_dynamic_cache(config, X₀, particle_dynamic)\n\nmethod_cache = particle_dynamic_cache.method_cache\n\ninitialise_particle_dynamic_cache!(X₀, particle_dynamic, particle_dynamic_cache)\ninitialise_dynamic!(particle_dynamic, particle_dynamic_cache)\ncompute_dynamic!(particle_dynamic, particle_dynamic_cache)\nfinalise_dynamic!(particle_dynamic, particle_dynamic_cache)\n\nout = wrap_output(X₀, particle_dynamic, particle_dynamic_cache)\n\nout.minimiser # should be close to [1, 1]","category":"page"},{"location":"low_level_examples/#Manual-stepping","page":"Low-level interface examples","title":"Manual stepping","text":"","category":"section"},{"location":"low_level_examples/","page":"Low-level interface examples","title":"Low-level interface examples","text":"This bypasses the compute_dynamic! method, performing the stepping manually instead:","category":"page"},{"location":"low_level_examples/","page":"Low-level interface examples","title":"Low-level interface examples","text":"details: Full example\nusing ConsensusBasedX, ConsensusBasedX.ConsensusBasedXLowLevel\n\nconfig =\n  (; D = 2, N = 20, M = 1, α = 10.0, λ = 1.0, σ = 1.0, Δt = 0.1, verbosity = 0)\n\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\n\nX₀ = [[rand(config.D) for n ∈ 1:(config.N)] for m ∈ 1:(config.M)]\n\ncorrection = HeavisideCorrection()\nnoise = IsotropicNoise\nmethod =\n  ConsensusBasedOptimisation(f, correction, noise, config.α, config.λ, config.σ)\n\nΔt = 0.1\nparticle_dynamic = ParticleDynamic(method, Δt)\n\nparticle_dynamic_cache =\n  construct_particle_dynamic_cache(config, X₀, particle_dynamic)\n\nmethod_cache = particle_dynamic_cache.method_cache\n\ninitialise_particle_dynamic_cache!(X₀, particle_dynamic, particle_dynamic_cache)\ninitialise_dynamic!(particle_dynamic, particle_dynamic_cache)\n\nfor it ∈ 1:100\n  for m ∈ 1:(config.M)\n    compute_dynamic_step!(particle_dynamic, particle_dynamic_cache, m)\n  end\nend\n\nfinalise_dynamic!(particle_dynamic, particle_dynamic_cache)\n\nout = wrap_output(X₀, particle_dynamic, particle_dynamic_cache)\n\nout.minimiser # should be close to [1, 1]","category":"page"},{"location":"low_level/#Low-level-interface","page":"Low-level interface","title":"Low-level interface","text":"","category":"section"},{"location":"low_level/","page":"Low-level interface","title":"Low-level interface","text":"Internally, the minimise routine relies on two constructs: the ParticleDynamic and a CBXMethod.","category":"page"},{"location":"low_level/#ConsensusBasedXLowLevel","page":"Low-level interface","title":"ConsensusBasedXLowLevel","text":"","category":"section"},{"location":"low_level/","page":"Low-level interface","title":"Low-level interface","text":"ConsensusBasedX.jl defines the ConsensusBasedXLowLevel submodule for the convenience of advanced users. It can be imported by","category":"page"},{"location":"low_level/","page":"Low-level interface","title":"Low-level interface","text":"using ConsensusBasedX, ConsensusBasedX.ConsensusBasedXLowLevel","category":"page"},{"location":"low_level/#ParticleDynamic","page":"Low-level interface","title":"ParticleDynamic","text":"","category":"section"},{"location":"low_level/","page":"Low-level interface","title":"Low-level interface","text":"The ParticleDynamic struct defines the evolution in time of particles, and is agnostic of the specific method in question. Its functionality currently serves ConsensusBasedOptimisation, but can be extended to other methods.","category":"page"},{"location":"low_level/#ConsensusBasedX.ParticleDynamic","page":"Low-level interface","title":"ConsensusBasedX.ParticleDynamic","text":"ParticleDynamic\n\nFields:\n\nmethod<:CBXMethod, the optimisation method.\nΔt::Float64, the time step.\n\n\n\n\n\n","category":"type"},{"location":"low_level/#ParticleDynamicCache","page":"Low-level interface","title":"ParticleDynamicCache","text":"","category":"section"},{"location":"low_level/","page":"Low-level interface","title":"Low-level interface","text":"ParticleDynamic requires a cache, ParticleDynamicCache. This can be constructed with:","category":"page"},{"location":"low_level/#ConsensusBasedX.construct_particle_dynamic_cache","page":"Low-level interface","title":"ConsensusBasedX.construct_particle_dynamic_cache","text":"construct_particle_dynamic_cache(\n  config::NamedTuple,\n  X₀::AbstractArray,\n  particle_dynamic::ParticleDynamic,\n)\n\nA constructor helper for ParticleDynamicCache. Calls ConsensusBasedX.construct_method_cache to construct the corresponding CBXMethodCache.\n\n\n\n\n\n","category":"function"},{"location":"low_level/","page":"Low-level interface","title":"Low-level interface","text":"The full reference is:","category":"page"},{"location":"low_level/#ConsensusBasedX.ParticleDynamicCache","page":"Low-level interface","title":"ConsensusBasedX.ParticleDynamicCache","text":"ParticleDynamicCache\n\nIt is strongly recommended that you do not construct ParticleDynamicCache by hand. Instead, use ConsensusBasedX.construct_particle_dynamic_cache.\n\nFields:\n\nmode should be set to ParticleMode.\nparallelisation<:Parallelisations, the parallelisation mode.\nmethod_cache<:CBXMethodCache, a cache for the method field of ParticleDynamic.\nD::Int, the dimension of the problem.\nN::Int, the number of particles per ensemble.\nM::Int, the number of ensembles.\nX, the particle array.\ndX, the time derivative array.\nΔt::Float64, the time step.\nroot_Δt::Float64, the square root of the time step.\nroot_2Δt::Float64, the square root of twice the time step.\nmax_iterations::Float64, the maximum number of iterations.\nmax_time::Float64, the maximal time.\niteration::Vector{Int}, the vector containing the iteration count per ensemble.\n\n\n\n\n\n","category":"type"},{"location":"low_level/#ConsensusBasedOptimisation","page":"Low-level interface","title":"ConsensusBasedOptimisation","text":"","category":"section"},{"location":"low_level/","page":"Low-level interface","title":"Low-level interface","text":"The ConsensusBasedOptimisation struct (of type CBXMethod) defines the details of the consensus-based optimisation method (function evaluations, consensus point...).","category":"page"},{"location":"low_level/#ConsensusBasedX.ConsensusBasedOptimisation","page":"Low-level interface","title":"ConsensusBasedX.ConsensusBasedOptimisation","text":"ConsensusBasedOptimisation\n\nFields:\n\nf, the objective function.\ncorrection<:CBXCorrection, a correction term.\nnoise<:Noises, a noise mode.\nα::Float64, the exponential weight parameter.\nλ::Float64, the drift strengh.\nσ::Float64, the noise strengh.\n\n\n\n\n\n","category":"type"},{"location":"low_level/#ConsensusBasedOptimisationCache","page":"Low-level interface","title":"ConsensusBasedOptimisationCache","text":"","category":"section"},{"location":"low_level/","page":"Low-level interface","title":"Low-level interface","text":"ConsensusBasedOptimisation requires a cache, ConsensusBasedOptimisationCache (of type CBXMethodCache). This can be constructed with:","category":"page"},{"location":"low_level/#ConsensusBasedX.construct_method_cache","page":"Low-level interface","title":"ConsensusBasedX.construct_method_cache","text":"construct_method_cache(\n  config::NamedTuple,\n  X₀::AbstractArray,\n  method::CBXMethod,\n  particle_dynamic::ParticleDynamic,\n)\n\nA constructor helper for CBXMethodCache.\n\n\n\n\n\n","category":"function"},{"location":"low_level/","page":"Low-level interface","title":"Low-level interface","text":"Note that this method is called automatically by ConsensusBasedX.construct_particle_dynamic_cache.","category":"page"},{"location":"low_level/","page":"Low-level interface","title":"Low-level interface","text":"The full reference is:","category":"page"},{"location":"low_level/#ConsensusBasedX.ConsensusBasedOptimisationCache","page":"Low-level interface","title":"ConsensusBasedX.ConsensusBasedOptimisationCache","text":"ConsensusBasedOptimisationCache{T}\n\nIt is strongly recommended that you do not construct ConsensusBasedOptimisationCache by hand. Instead, use ConsensusBasedX.construct_method_cache.\n\nFields:\n\nconsensus::Vector{Vector{T}}, the consensus point of each ensemble.\nconsensus_energy::Vector{T}, the energy (value of the objective function) of each consensus point.\nconsensus_energy_previous::Vector{T}, the previous energy.\ndistance::Vector{Vector{T}}, the distance of each particle to the consensus point.\nenergy::Vector{Vector{T}}, the energy of each particle.\nexponents::Vector{Vector{T}}, an exponent used to compute logsums.\nlogsums::Vector{T}, a normalisation factor for weights.\nweights::Vector{Vector{T}}, the exponential weight of each particle.\nenergy_threshold::Float64, the energy threshold.\nenergy_tolerance::Float64, the energy tolerance.\nmax_evaluations::Float64, the maximum number of f evaluations.\nevaluations::Vector{Int}, the current number of f evaluations.\n\n\n\n\n\n","category":"type"},{"location":"low_level/#ConsensusBasedSampling","page":"Low-level interface","title":"ConsensusBasedSampling","text":"","category":"section"},{"location":"low_level/","page":"Low-level interface","title":"Low-level interface","text":"The ConsensusBasedSampling struct (of type CBXMethod) defines the details of the consensus-based sampling method (function evaluations, covariance matrix...).","category":"page"},{"location":"low_level/#ConsensusBasedX.ConsensusBasedSampling","page":"Low-level interface","title":"ConsensusBasedX.ConsensusBasedSampling","text":"ConsensusBasedSampling\n\nFields:\n\nf, the objective function.\nroot<:Roots, a mode for the square toor of the covariance.\nα::Float64, the exponential weight parameter.\nλ::Float64, the mode parameter. λ = 1 / (1 + α) corresponds to CBS_mode = :sampling, and λ = 1 corresponds to CBS_mode = :minimise.\n\n\n\n\n\n","category":"type"},{"location":"low_level/#ConsensusBasedSamplingCache","page":"Low-level interface","title":"ConsensusBasedSamplingCache","text":"","category":"section"},{"location":"low_level/","page":"Low-level interface","title":"Low-level interface","text":"ConsensusBasedSampling requires a cache, ConsensusBasedSamplingCache (of type CBXMethodCache). This can be constructed with ConsensusBasedX.construct_method_cache.","category":"page"},{"location":"low_level/","page":"Low-level interface","title":"Low-level interface","text":"The full reference is:","category":"page"},{"location":"low_level/#ConsensusBasedX.ConsensusBasedSamplingCache","page":"Low-level interface","title":"ConsensusBasedX.ConsensusBasedSamplingCache","text":"ConsensusBasedSamplingCache{T}\n\nIt is strongly recommended that you do not construct ConsensusBasedSamplingCache by hand. Instead, use ConsensusBasedX.construct_method_cache.\n\nFields:\n\nconsensus::Vector{Vector{T}}, the consensus point of each ensemble.\nconsensus_energy::Vector{T}, the energy (value of the objective function) of each consensus point.\nconsensus_energy_previous::Vector{T}, the previous energy.\nenergy::Vector{Vector{T}}, the energy of each particle.\nexponents::Vector{Vector{T}}, an exponent used to compute logsums.\nlogsums::Vector{T}, a normalisation factor for weights.\nnoise::Vector{Vector{T}}, a vector to contain the noise of one iteration.\nroot_covariance::Vector{Matrix{T}}, the matrix square root of the weighted covariance of the particles.\nweights::Vector{Vector{T}}, the exponential weight of each particle.\nenergy_threshold::Float64, the energy threshold.\nenergy_tolerance::Float64, the energy tolerance.\nmax_evaluations::Float64, the maximum number of f evaluations.\nevaluations::Vector{Int}, the current number of f evaluations.\nexp_minus_Δt::Float64, the time-stepping parameter.\nnoise_factor::Float64, the noise multiplier.\n\n\n\n\n\n","category":"type"},{"location":"distribution_sampling/#Distribution-sampling","page":"Distribution sampling","title":"Distribution sampling","text":"","category":"section"},{"location":"distribution_sampling/","page":"Distribution sampling","title":"Distribution sampling","text":"ConsensusBasedX.jl also provides Consensus-Based Sampling.","category":"page"},{"location":"distribution_sampling/","page":"Distribution sampling","title":"Distribution sampling","text":"The package exports sample, which behaves exactly as minimise in Function minimisation. It assumes you have defined a function f(x::AbstractVector) that takes a single vector argument x of length D = length(x).","category":"page"},{"location":"distribution_sampling/","page":"Distribution sampling","title":"Distribution sampling","text":"For instance, if D = 2, you can sample exp(-αf) by running:","category":"page"},{"location":"distribution_sampling/","page":"Distribution sampling","title":"Distribution sampling","text":"out = sample(f, D = 2, extended_output=true)\nout.sample","category":"page"},{"location":"distribution_sampling/","page":"Distribution sampling","title":"Distribution sampling","text":"The method must be run with the extended_output=true option in order to receive Extended output and access the full particle sample; without it, sample returns a single Vector{Float64} of length D which contains the candidate to the global minimiser of f, just like minimise.","category":"page"},{"location":"distribution_sampling/","page":"Distribution sampling","title":"Distribution sampling","text":"note: Note\nYou must always provide D.","category":"page"},{"location":"distribution_sampling/","page":"Distribution sampling","title":"Distribution sampling","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nout = sample(f, D = 2, N = 20, extended_output = true)\nout.sample","category":"page"},{"location":"distribution_sampling/#Using-a-config-object","page":"Distribution sampling","title":"Using a config object","text":"","category":"section"},{"location":"distribution_sampling/","page":"Distribution sampling","title":"Distribution sampling","text":"For more advanced usage, you will select several options. You can pass these as extra keyword arguments to sample, or you can create a NamedTuple called config and pass that:","category":"page"},{"location":"distribution_sampling/","page":"Distribution sampling","title":"Distribution sampling","text":"config = (; D = 2, extended_output=true)\nout = sample(f, config)\nout.sample","category":"page"},{"location":"distribution_sampling/","page":"Distribution sampling","title":"Distribution sampling","text":"note: Note\nIf you pass a Dict instead, it will be converted to a NamedTuple automatically.","category":"page"},{"location":"distribution_sampling/","page":"Distribution sampling","title":"Distribution sampling","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nconfig = (; D = 2, N = 20, extended_output = true)\nout = sample(f, config)\nout.sample","category":"page"},{"location":"distribution_sampling/#Running-on-minimisation-mode","page":"Distribution sampling","title":"Running on minimisation mode","text":"","category":"section"},{"location":"distribution_sampling/","page":"Distribution sampling","title":"Distribution sampling","text":"Consensus-based sampling can also be used for minimisation (see Consensus-Based Sampling). If you want to run it in that mode, pass the option CBS_mode = :minimise.","category":"page"},{"location":"distribution_sampling/#Method-reference","page":"Distribution sampling","title":"Method reference","text":"","category":"section"},{"location":"distribution_sampling/","page":"Distribution sampling","title":"Distribution sampling","text":"Pages = [\"distribution_sampling.md\"]","category":"page"},{"location":"distribution_sampling/#ConsensusBasedX.sample","page":"Distribution sampling","title":"ConsensusBasedX.sample","text":"sample(f; keywords...)\n\nsample(f, config::NamedTuple)\n\nSample the distribution exp(-αf) using Consensus-Based Sampling (see Distribution sampling).\n\nYou must specify the dimension D of the problem. Other paramters (e.g. the number of particles N or the number of ensembles M) can also be specified; see Summary of options.\n\nExamples\n\nout = sample(f, D = 2, extended_output = true);\nout.sample\n\nconfig = (; D = 2, extended_output = true);\nout = sample(f, config);\nout.sample\n\nout = sample(f, D = 2, N = 20, extended_output = true);\nout.sample\n\nconfig = (; D = 2, N = 20, extended_output = true);\nout = sample(f, config);\nout.sample\n\n\n\n\n\n","category":"function"},{"location":"example_objectives/#Example-objectives","page":"Example objectives","title":"Example objectives","text":"","category":"section"},{"location":"example_objectives/","page":"Example objectives","title":"Example objectives","text":"ConsensusBasedX.jl provides a few example functions that can be used to test the minimisation algorithms and parameters.","category":"page"},{"location":"example_objectives/","page":"Example objectives","title":"Example objectives","text":"Pages = [\"example_objectives.md\"]","category":"page"},{"location":"example_objectives/#ConsensusBasedX.Ackley","page":"Example objectives","title":"ConsensusBasedX.Ackley","text":"Ackley(x::AbstractVector; a = 20, b = 0.2, c = 2π, shift = 0)\n\nThe Ackley function in dimension D = length(x) with global minimum at the point x = (textrmshift textrmshift cdots textrmshift):\n\nf(x) =\na left\n1 - exp left(\n-b sqrt\nfrac1D\nsum_d=1^D (x_d - textrmshift)^2\n\nright)\nright\n+ left\nexp(1)\n-\nexp left(\nfrac1D\nsum_d=1^D cos( c (x_d - textrmshift) )\nright)\nright\n\n\nSee also the Wikipedia article.\n\n\n\n\n\n","category":"function"},{"location":"example_objectives/#ConsensusBasedX.Quadratic","page":"Example objectives","title":"ConsensusBasedX.Quadratic","text":"Quadratic(x::AbstractVector; shift = 0)\n\nA quadratic function in dimension D = length(x) with global minimum at the point x = (textrmshift textrmshift cdots textrmshift):\n\nf(x) =\nfrac1D\nsum_d=1^D (x_d - textrmshift)^2\n\n\n\n\n\n\n","category":"function"},{"location":"example_objectives/#ConsensusBasedX.Rastrigin","page":"Example objectives","title":"ConsensusBasedX.Rastrigin","text":"Rastrigin(x::AbstractVector; a = 10, c = 2π, shift = 0)\n\nThe Rastrigin function in dimension D = length(x) with global minimum at the point x = (textrmshift textrmshift cdots textrmshift):\n\nf(x) =\nfrac1D\nsum_d=1^D (x_d - textrmshift)^2\n+ a left(\n1\n-\nfrac1D\nsum_d=1^D cos( c (x_d - textrmshift) )\nright)\n\n\nSee also the Wikipedia article.\n\n\n\n\n\n","category":"function"},{"location":"method_parameters/#Method-parameters","page":"Method parameters","title":"Method parameters","text":"","category":"section"},{"location":"method_parameters/","page":"Method parameters","title":"Method parameters","text":"You can change any of these parameters by passing them as keywords to the minimise routine.","category":"page"},{"location":"method_parameters/#Number-of-particles","page":"Method parameters","title":"Number of particles","text":"","category":"section"},{"location":"method_parameters/","page":"Method parameters","title":"Method parameters","text":"ConsensusBasedX.jl uses N = 20 particles by default. At each iteration of the method, the function f is evaluated at the position of each particle.","category":"page"},{"location":"method_parameters/","page":"Method parameters","title":"Method parameters","text":"tip: Tip\nUsing more particles is likely to yield better results, but it will increase the computational cost of the minimisation.","category":"page"},{"location":"method_parameters/#Number-of-ensembles","page":"Method parameters","title":"Number of ensembles","text":"","category":"section"},{"location":"method_parameters/","page":"Method parameters","title":"Method parameters","text":"ConsensusBasedX.jl uses M = 1 ensembles by default. Each ensemble will perform its own minimisation of f, and the final results will be averaged.","category":"page"},{"location":"method_parameters/","page":"Method parameters","title":"Method parameters","text":"tip: Tip\nUsing more ensembles is likely to yield better results, but it will increase the computational cost of the minimisation.","category":"page"},{"location":"method_parameters/#Time-step","page":"Method parameters","title":"Time step","text":"","category":"section"},{"location":"method_parameters/","page":"Method parameters","title":"Method parameters","text":"ConsensusBasedX.jl uses a time step of Δt = 0.1 by default. The evolution of the particles is given by a certain stochastic differential equation, which is solved using an Euler-Maruyama scheme with time step Δt.","category":"page"},{"location":"method_parameters/","page":"Method parameters","title":"Method parameters","text":"tip: Tip\nReducing Δt will require more iterations of the method to converge, but will provide additional stability. Reduce Δt only if your minimisation \"blows up\" (returns unusually large numbers, Inf, or NaN).","category":"page"},{"location":"method_parameters/#Consensus-parameters","page":"Method parameters","title":"Consensus parameters","text":"","category":"section"},{"location":"method_parameters/","page":"Method parameters","title":"Method parameters","text":"Consensus-based optimisation requires three parameters:","category":"page"},{"location":"method_parameters/","page":"Method parameters","title":"Method parameters","text":"σ::Real = 1 is the noise strengh;\nλ::Real = 1 is the drift strengh;\nα::Real = 10 is the exponential weight parameter.","category":"page"},{"location":"method_parameters/","page":"Method parameters","title":"Method parameters","text":"tip: Tip\nA low value of σ and a high value of λ make the particles converge towards the consensus point more directly; this is a good idea if you have a very good guess for the global minimiser (see Particle initialisation). A high value of σ and a low value of λ make the particles explore more of the landscape before converging, which is useful if your initial guess is bad. Similarly, a higher value of α biases the consensus point towards the current best particle, which is only desirable if your initial guess is good.","category":"page"},{"location":"method_parameters/","page":"Method parameters","title":"Method parameters","text":"details: Full example\nusing ConsensusBasedX\nf(x) = ConsensusBasedX.Ackley(x, shift = 1)\nconfig = (; D = 2, N = 20, M = 1, Δt = 0.01, σ = 1, λ = 1, α = 10)\nminimise(f, config) # should be close to [1, 1]","category":"page"},{"location":"#ConsensusBasedX.jl:-Consensus-Based-Optimisation-in-Julia","page":"Home","title":"ConsensusBasedX.jl: Consensus-Based Optimisation in Julia","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"<iframe src=\"https://ghbtns.com/github-btn.html?user=PdIPS&repo=ConsensusBasedX.jl&type=star&count=true&size=large\" frameborder=\"0\" scrolling=\"0\" width=\"170\" height=\"30\" title=\"GitHub\"></iframe>","category":"page"},{"location":"","page":"Home","title":"Home","text":"ConsensusBasedX.jl is a gradient-free stochastic optimisation package for Julia, born out of Consensus.jl and CBXpy. It uses Consensus-Based Optimisation (CBO), a flavour of Particle Swarm Optimisation (PSO) first introduced by R. Pinnau, C. Totzeck, O. Tse, and S. Martin (2017). This is a method of global optimisation particularly suited for rough functions, where gradient descent would fail. It is useful for optimisation in higher dimensions. It also implements Consensus-Based Sampling (CBS), as introduced in J. A. Carrillo, F. Hoffmann, A. M. Stuart, and U. Vaes (2022). ","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Static Badge) (Image: Build Status) (Image: Coverage) (Image: Aqua) (Image: License: MIT)","category":"page"},{"location":"#How-to-install-and-use","page":"Home","title":"How to install and use","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install ConsensusBasedX.jl, simply run","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg; Pkg.add(\"ConsensusBasedX\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"in the Julia REPL. You can then load the package in a script or in the REPL by running","category":"page"},{"location":"","page":"Home","title":"Home","text":"using ConsensusBasedX","category":"page"},{"location":"#Basic-minimisation","page":"Home","title":"Basic minimisation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The main functionality of ConsensusBasedX.jl is function minimisation via CBO. It assumes you have defined a function f(x::AbstractVector) that takes a single vector argument x of length D = length(x).","category":"page"},{"location":"","page":"Home","title":"Home","text":"For instance, if D = 2, you can minimise f by running:","category":"page"},{"location":"","page":"Home","title":"Home","text":"minimise(f, D = 2)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Your full code might look like this:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using ConsensusBasedX\nf(x) = x[1]^2 + x[2]^2\nx = minimise(f, D = 2)","category":"page"},{"location":"#Basic-sampling","page":"Home","title":"Basic sampling","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ConsensusBasedX.jl also provides CBS. The package exports sample, which has the same syntax as minimise.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For instance, if D = 2, you can sample exp(-αf) by running:","category":"page"},{"location":"","page":"Home","title":"Home","text":"out = sample(f, D = 2, extended_output=true)\nout.sample","category":"page"},{"location":"#Bug-reports,-feature-requests,-and-contributions","page":"Home","title":"Bug reports, feature requests, and contributions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See the contribution guidelines.","category":"page"}]
}
